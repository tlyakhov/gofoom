// Code generated by command: go run blend_asm.go -out blend_asm.s -stubs blend_asm_stub.go. DO NOT EDIT.

#include "textflag.h"

DATA data<>+0(SB)/8, $0x3ff0000000000000
DATA data<>+8(SB)/8, $0x406fe00000000000
DATA data<>+16(SB)/8, $0x0000000000010203
DATA data<>+24(SB)/8, $0x3f70101010101010
GLOBL data<>(SB), RODATA|NOPTR, $32

// func BlendFrameBuffer(target []uint8, fb [][4]float64, tint *[4]float64)
// Requires: AVX
TEXT ·BlendFrameBuffer(SB), NOSPLIT, $0-56
	MOVQ fb_base+24(FP), AX
	MOVQ target_base+0(FP), CX
	MOVQ target_len+8(FP), DX
	MOVQ tint+48(FP), BX

	// Set up a zero register
	VXORPS Y0, Y0, Y0

	// Broadcast 1.0 into a register
	VBROADCASTSD data<>+0(SB), Y1

	// Broadcast 255.0 into a register
	VBROADCASTSD data<>+8(SB), Y2

	// tintPacked = *tintPtr
	VMOVUPD (BX), Y3

	// tintPacked = MIN(tintPacked,1)
	VMINPD Y3, Y1, Y3

	// tintPacked = MAX(tintPacked,0)
	VMAXPD Y3, Y0, Y3

	// Put the alpha value tint[3] into a register
	VBROADCASTSD 24(BX), Y0

	// If we have FMA, we could just do VFMADD132PD(b, bAlpha, a)
	// but I want to target general SSE/AVX
	// bAlpha = 1.0 - bAlpha
	VSUBPD Y0, Y1, Y0

	// FB ptr register
LoopStart:
	CMPQ DX, $0x00
	JE   LoopEnd

	// fbPacked = *fbPtr
	VMOVUPD (AX), Y1

	// fbPacked = fbPacked * tintAlpha
	VMULPD Y1, Y0, Y1

	// fbPacked = fbPacked + tintPacked
	VADDPD Y3, Y1, Y1

	// fbPacked *= 255
	VMULPD Y1, Y2, Y1

	// Convert float64 -> int32
	VCVTPD2DQY Y1, X1

	// int32 -> int16
	VPACKUSDW X1, X1, X1

	// int16 -> int8
	VPACKUSWB X1, X1, X1
	VMOVD     X1, (CX)

	// Increment pointers
	ADDQ $0x20, AX
	ADDQ $0x04, CX

	// fbPacked = *fbPtr
	VMOVUPD (AX), Y1

	// fbPacked = fbPacked * tintAlpha
	VMULPD Y1, Y0, Y1

	// fbPacked = fbPacked + tintPacked
	VADDPD Y3, Y1, Y1

	// fbPacked *= 255
	VMULPD Y1, Y2, Y1

	// Convert float64 -> int32
	VCVTPD2DQY Y1, X1

	// int32 -> int16
	VPACKUSDW X1, X1, X1

	// int16 -> int8
	VPACKUSWB X1, X1, X1
	VMOVD     X1, (CX)

	// Increment pointers
	ADDQ $0x20, AX
	ADDQ $0x04, CX

	// fbPacked = *fbPtr
	VMOVUPD (AX), Y1

	// fbPacked = fbPacked * tintAlpha
	VMULPD Y1, Y0, Y1

	// fbPacked = fbPacked + tintPacked
	VADDPD Y3, Y1, Y1

	// fbPacked *= 255
	VMULPD Y1, Y2, Y1

	// Convert float64 -> int32
	VCVTPD2DQY Y1, X1

	// int32 -> int16
	VPACKUSDW X1, X1, X1

	// int16 -> int8
	VPACKUSWB X1, X1, X1
	VMOVD     X1, (CX)

	// Increment pointers
	ADDQ $0x20, AX
	ADDQ $0x04, CX

	// fbPacked = *fbPtr
	VMOVUPD (AX), Y1

	// fbPacked = fbPacked * tintAlpha
	VMULPD Y1, Y0, Y1

	// fbPacked = fbPacked + tintPacked
	VADDPD Y3, Y1, Y1

	// fbPacked *= 255
	VMULPD Y1, Y2, Y1

	// Convert float64 -> int32
	VCVTPD2DQY Y1, X1

	// int32 -> int16
	VPACKUSDW X1, X1, X1

	// int16 -> int8
	VPACKUSWB X1, X1, X1
	VMOVD     X1, (CX)

	// Increment pointers
	ADDQ $0x20, AX
	ADDQ $0x04, CX

	// Decrement loop counter
	SUBQ $0x10, DX
	JMP  LoopStart

LoopEnd:
	// This is critical to avoid slowdowns when mixing SSE/AVX code. See
	// Intel Architectures Optimization Reference Manual Volume 1
	// section 3.11.6.3 "Fixing Instruction Sequence Slowdowns"
	VZEROUPPER
	RET

	// Benchmarked call overhead is ~2.8 ns/op

// func BlendColors(a *[4]float64, b *[4]float64, opacity float64)
// Requires: AVX
TEXT ·BlendColors(SB), NOSPLIT, $0-24
	MOVQ a+0(FP), AX
	MOVQ b+8(FP), CX

	// setup a zero register
	VXORPS Y0, Y0, Y0

	// Broadcast 1.0 into a register
	VBROADCASTSD data<>+0(SB), Y1

	// opacityPacked = opacity,opacity,opacity,opacity
	VBROADCASTSD opacity+16(FP), Y2

	// Put the alpha value b[3] into a register
	VBROADCASTSD 24(CX), Y3

	// bAlpha *= opacity
	VMULPD Y2, Y3, Y3

	// If we have FMA, we could just do VFMADD132PD(b, bAlpha, a)
	// but I want to target general SSE/AVX
	// bAlpha = 1.0 - bAlpha
	VSUBPD Y3, Y1, Y3

	// bPacked = *bptr
	VMOVUPD (CX), Y4

	// bPacked = *bptr * opacity
	VMULPD Y2, Y4, Y4

	// bPacked = MIN(bPacked,1)
	VMINPD Y4, Y1, Y4

	// bPacked = MAX(bPacked,0)
	VMAXPD Y4, Y0, Y4

	// aPacked = a
	VMOVUPD (AX), Y2

	// aPacked = aPacked * bAlpha
	VMULPD Y2, Y3, Y2

	// aPacked = aPacked + bPacked
	VADDPD Y4, Y2, Y2

	// aPacked = MIN(aPacked,1)
	VMINPD Y2, Y1, Y2

	// aPacked = MAX(aPacked,0)
	VMAXPD Y2, Y0, Y2

	// *aptr = aPacked
	VMOVUPD Y2, (AX)

	// This is critical to avoid slowdowns when mixing SSE/AVX code. See
	// Intel Architectures Optimization Reference Manual Volume 1
	// section 3.11.6.3 "Fixing Instruction Sequence Slowdowns"
	VZEROUPPER
	RET

// func AsmVector4Mul4Self(a *[4]float64, b *[4]float64)
// Requires: AVX
TEXT ·AsmVector4Mul4Self(SB), NOSPLIT, $0-16
	MOVQ a+0(FP), AX
	MOVQ b+8(FP), CX

	// a = *aptr
	VMOVUPD (AX), Y0

	// a = a * *bptr
	VMOVUPD (CX), Y1
	VMULPD  Y1, Y0, Y0
	VMOVUPD Y0, (AX)
	RET

// func AsmInt32ToVector4(c uint32, a *[4]float64)
// Requires: AVX, SSE2, SSE4.1, SSSE3
TEXT ·AsmInt32ToVector4(SB), NOSPLIT, $0-16
	MOVQ c+0(FP), X0
	MOVQ a+8(FP), AX

	// Move quadword from r/m64 to xmm1.
	VMOVQ data<>+8(SB), X1

	// Broadcast 1.0/255.0 into a register
	VBROADCASTSD data<>+16(SB), Y2

	// Shuffle bytes in xmm1 according to contents of xmm2/m128.
	PSHUFB X1, X0

	// Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm
	PMOVZXBD X0, X0

	// Convert four packed signed doubleword integers from xmm2/mem to four packed double precision floating-point values in ymm1.
	VCVTDQ2PD X0, Y0

	// Multiply packed double precision floating-point values in ymm3/m256 with
	// ymm2 and store result in ymm1.
	VMULPD Y0, Y2, Y0

	// Move unaligned packed double precision floating-point from ymm1 to ymm2/mem.
	VMOVUPD Y0, (AX)

	// This is critical to avoid slowdowns when mixing SSE/AVX code. See
	// Intel Architectures Optimization Reference Manual Volume 1
	// section 3.11.6.3 "Fixing Instruction Sequence Slowdowns"
	VZEROUPPER
	RET

// func AsmInt32ToVector4PreMul(c uint32, a *[4]float64)
// Requires: AVX, SSE2, SSE4.1, SSSE3
TEXT ·AsmInt32ToVector4PreMul(SB), NOSPLIT, $0-16
	MOVQ c+0(FP), X0
	MOVQ a+8(FP), AX

	// Move quadword from r/m64 to xmm1.
	VMOVQ data<>+8(SB), X1

	// Broadcast 1.0/255.0 into a register
	VBROADCASTSD data<>+16(SB), Y2

	// Shuffle bytes in xmm1 according to contents of xmm2/m128.
	PSHUFB X1, X0

	// Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm
	PMOVZXBD X0, X0

	// Convert four packed signed doubleword integers from xmm2/mem to four packed double precision floating-point values in ymm1.
	VCVTDQ2PD X0, Y0

	// Multiply packed double precision floating-point values in ymm3/m256 with
	// ymm2 and store result in ymm1.
	VMULPD Y0, Y2, Y0

	// Move unaligned packed double precision floating-point from ymm1 to ymm2/mem.
	VMOVUPD Y0, (AX)

	// This is critical to avoid slowdowns when mixing SSE/AVX code. See
	// Intel Architectures Optimization Reference Manual Volume 1
	// section 3.11.6.3 "Fixing Instruction Sequence Slowdowns"
	VZEROUPPER
	RET
